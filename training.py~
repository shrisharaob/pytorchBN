import torch
import torch.nn as nn
import torch.nn.functional as F


class BalRNN(nn.Module):
    '''one pop bal network'''

    def __init__(self,
                 input_size,
                 hidden_size,
                 num_layers,
                 batch_first=True,
                 K=10,
                 J0I=0.4,
                 JII=-0.1):
        super(BalRNN, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.batch_first = batch_first
        self.K = K
        self.J0I = J0I
        self.JII = JII
        #
        self.history = []
        #
        self.weight_ih = nn.ParameterList([
            nn.Parameter(
                torch.randn(hidden_size, input_size) *
                (J0I / torch.sqrt(torch.tensor(K, dtype=torch.float32))))
            for _ in range(num_layers)
        ])

        self.weight_hh = nn.ParameterList([
            nn.Parameter(self.initialize_sparse_weight(hidden_size, K, JII))
            for _ in range(num_layers)
        ])

    def initialize_sparse_weight(self, hidden_size, K, J):
        indices = []
        values = []
        for i in range(hidden_size):
            selected_indices = torch.randperm(hidden_size)[:int(
                K**0.5)]  # Choose sqrt(K) indices randomly
            indices.extend([[i, j] for j in selected_indices])
            values.extend(
                [J / torch.sqrt(torch.tensor(K, dtype=torch.float32))] *
                int(K**0.5))

        indices = torch.tensor(indices).t()
        values = torch.tensor(values, dtype=torch.float32)
        sparse_weight = torch.sparse.FloatTensor(
            indices, values, torch.Size([hidden_size, hidden_size]))
        return sparse_weight

    def transfer_function(self, total_input):
        return F.relu(total_input)

    def forward(self, x, h_0=None):
        if self.batch_first:
            x = x.transpose(0, 1)  # Ensure (seq_len, batch_size, input_size)
        seq_len, batch_size, _ = x.size()
        if h_0 is None:
            h_0 = torch.zeros(self.num_layers, batch_size, self.hidden_size)
        h_t_minus_1 = h_0
        h_t = h_0.clone()
        output = []
        for t in range(seq_len):
            for layer in range(self.num_layers):
                if layer == 0:
                    h_t[layer] = self.transfer_function(
                        torch.sparse.mm(self.weight_ih[layer], x[t].T).T +
                        torch.sparse.mm(self.weight_hh[layer],
                                        h_t_minus_1[layer].T).T)
                else:
                    h_t[layer] = self.transfer_function(
                        torch.sparse.mm(self.weight_hh[layer], h_t[layer -
                                                                   1].T).T +
                        torch.sparse.mm(self.weight_hh[layer],
                                        h_t_minus_1[layer].T).T)
            output.append(h_t[-1])
            h_t_minus_1 = h_t.clone()
        output = torch.stack(output)
        if self.batch_first:
            output = output.transpose(0, 1)
        return output, h_t

    def mask_non_trainable_weights(self):
        for layer in range(self.num_layers):
            weight_hh_layer = self.weight_hh[layer]
            weight_hh_layer.data.mul_(weight_hh_layer._mask)


# Example Usage:
input_size = 10
hidden_size = 5
num_layers = 2
batch_size = 3
seq_len = 4

model = BalRNN(input_size, hidden_size, num_layers)

# Mask non-trainable weights
for layer in range(num_layers):
    weight_hh_layer = model.weight_hh[layer]
    selected_indices = torch.randperm(hidden_size)[:int(hidden_size**0.5)]
    mask = torch.zeros(hidden_size, hidden_size)
    mask[:, selected_indices] = 1
    weight_hh_layer._mask = nn.Parameter(mask, requires_grad=False)

input_data = torch.randn(seq_len, batch_size, input_size)
output, _ = model(input_data)
print(output.shape)  # Output shape: (seq_len, batch_size, hidden_size)
